
# 報告：利用神經網路近似 Runge Function

## 前言(Preface)

在本次測試中，嘗試使用 Stochastic Gradient Descent (SGD) 作為優化器，但效果不理想。
數據顯示，SGD 的 MSE 與最大誤差皆高於 Adam，近似函數的效果也不如 Adam 平穩。因此，本次作業最終選擇使用 Adam 作為優化器，以獲得更佳的收斂效率與近似效果。

## 方法 (Method)

此程式的目標是使用一個前饋式神經網路（Multi-Layer Perceptron, MLP）來近似 Runge function：
$$
f(x) = \frac{1}{1+25x^2}, \quad x\in[-1, 1].
$$
主要步驟如下：

1. **資料準備**  
    在區間 [−1,1] 均勻取 200 個點，計算對應的 Runge function 值。接著將資料分為訓練集（80%）與驗證集（20%）。
    
2. **模型架構**  
    採用單隱藏層 MLP，結構如下：
    
    - 輸入層：1 維
        
    - 隱藏層：100 個神經元，啟動函數使用 ReLU
        
    - 輸出層：1 維
        
3. **訓練方式**
    
    - 損失函數：MSELoss
        
    - 優化器：Adam，學習率 0.01
        
    - 訓練 500 個 epochs，並同時記錄訓練集與驗證集的 loss。
        
4. **評估指標**
    
    - 均方誤差（MSE）
        
    - 最大誤差（Max Error）
        

---

## 結果 (Results)

1. **函數近似效果**  
    如圖 1 所示，神經網路預測曲線（橘色虛線）與真實 Runge function（藍色實線）高度吻合，特別是在區間內大部分位置，僅在中心點附近有輕微偏差。
    
    **圖 1. ![[Pasted image 20250914213641.png]] 
    
2. **訓練與驗證誤差**  
    如圖 2 所示，訓練損失與驗證損失皆快速下降，並在約 100 個 epoch 後趨近於零，顯示模型成功學習到函數形式，且無明顯 overfitting。
    
    **圖 2. ![[Pasted image 20250914213732.png]]
    
3. **ReLU 與導數驗證**  
    額外分析 ReLU 的自動微分結果，與理論值完全吻合。這也驗證了 PyTorch 自動微分的正確性。
    
    **圖 3. ![[Pasted image 20250914213822.png]]
    
4. **誤差指標**
    在 ReLU' 的驗證部分，五次實驗結果顯示 **自動微分 (autograd)** 與 **理論導數** 的一致性極佳。
    - **均方誤差 (MSE)** 均維持在$10^{-5}$的量級($3.2 \times 10^{-5} \sim 6.8 \times 10^{-5}）$。
    - **最大誤差 (Max Error)** 落在 0.016∼0.042 區間，屬於非常小的偏差。
    - **準確率 (Accuracy)** 在五次測試中皆為 **1.0**，代表在數值層級上兩者幾乎完全吻合。
        

---

## 討論 (Discussion)

1. **模型表現**  
    實驗結果顯示單隱藏層的 MLP 已能夠成功近似 Runge function，這說明即使簡單的結構，在非線性激活函數（ReLU）的幫助下，也具有良好的函數逼近能力。
    
2. **誤差來源**  
    雖然整體近似良好，但在函數變化最劇烈的 x=0 附近，仍觀察到最大誤差。這是因為 Runge function 在此處梯度較大，模型需要更多參數或更深層結構才能更精確擬合。
    
3. **改進方向**
    
    - 增加網路深度（例如兩層隱藏層）。
        
    - 嘗試其他啟動函數（如 Tanh），以獲得更平滑的近似。

---

## 結論

此程式成功地使用神經網路近似 Runge function，並透過圖表與誤差指標驗證模型的有效性。結果顯示，即便是簡單的單隱藏層 ReLU 網路，也能在 [−1,1] 區間內準確擬合該函數，達到良好的近似效果。